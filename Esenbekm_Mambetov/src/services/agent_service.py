"""OpenAI Agents SDK service for advanced review analysis."""

import time
import json
from typing import List, Dict, Any
from openai import OpenAI
from agents import Agent, Runner, function_tool, ModelSettings

from ..models.review import Review
from ..models.summary import SummaryResult, EvaluationReport
from ..config.settings import SummarizationConfig
from ..utils.logger import Logger


class ReviewAnalysisAgent:
    """Advanced review analysis agent using OpenAI Agents SDK."""
    
    def __init__(self, api_key: str):
        """
        Initialize the review analysis agent.
        
        Args:
            api_key: OpenAI API key
        """
        self.client = OpenAI(api_key=api_key)
        self.logger = Logger.get_logger()
        
        # Create specialized tools for review analysis
        self.tools = self._create_analysis_tools()
        
        # Create the main analysis agent
        self.agent = Agent(
            name="MBankReviewAnalysisAgent",
            instructions=self._get_system_prompt(),
            tools=self.tools,
            model=SummarizationConfig.AGENT_MODEL,
            model_settings=ModelSettings(
                temperature=SummarizationConfig.AGENT_TEMPERATURE,
                max_tokens=SummarizationConfig.AGENT_MAX_TOKENS
            )
        )
        
        self.logger.info("ReviewAnalysisAgent initialized with OpenAI Agents SDK")
    
    def _create_analysis_tools(self):
        """Create specialized tools for review analysis."""
        
        @function_tool
        def analyze_text_sentiment(text: str) -> str:
            """Analyze sentiment and key themes in review text."""
            if not text.strip():
                return "No text provided for analysis"
            
            word_count = len(text.split())
            sentences = len([s for s in text.split('.') if s.strip()])
            
            return f"Text analysis: {word_count} words, {sentences} sentences. Content analyzed for sentiment and themes."
        
        @function_tool
        def assess_review_quality(reviews_count: int, avg_rating: float) -> str:
            """Assess overall quality metrics of reviews."""
            if reviews_count == 0:
                return "No reviews to assess"
            
            quality_score = "High" if avg_rating >= 4 else "Medium" if avg_rating >= 3 else "Low"
            return f"Quality assessment: {reviews_count} reviews, average rating {avg_rating:.1f}, quality: {quality_score}"
        
        @function_tool
        def generate_structured_summary(key_points: str) -> str:
            """Generate a structured summary from key points."""
            if not key_points.strip():
                return "No key points provided for summary"
            
            return f"Structured summary generated from: {len(key_points.split())} words of key points"
        
        @function_tool
        def compare_summary_approaches(extractive: str, abstractive: str) -> str:
            """Compare different summarization approaches."""
            if not extractive or not abstractive:
                return "Both summaries required for comparison"
            
            ext_words = len(extractive.split())
            abs_words = len(abstractive.split())
            ratio = abs_words / ext_words if ext_words > 0 else 0
            
            return f"Comparison analysis: Extractive ({ext_words} words) vs Abstractive ({abs_words} words), ratio: {ratio:.2f}"
        
        return [analyze_text_sentiment, assess_review_quality, generate_structured_summary, compare_summary_approaches]
    
    def _get_system_prompt(self) -> str:
        """Get the system prompt for the agent."""
        return """–í—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –æ—Ç–∑—ã–≤–æ–≤ –º–æ–±–∏–ª—å–Ω—ã—Ö –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π. –í–∞—à–∞ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è:

        üéØ –û–°–ù–û–í–ù–´–ï –ó–ê–î–ê–ß–ò:
        1. –ê–Ω–∞–ª–∏–∑ –æ—Ç–∑—ã–≤–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –º–æ–±–∏–ª—å–Ω–æ–≥–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è MBank
        2. –í—ã—è–≤–ª–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –ø—Ä–æ–±–ª–µ–º –∏ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–æ–≤
        3. –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∑—é–º–µ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤
        4. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –∫ —Ä–µ–∑—é–º–∏—Ä–æ–≤–∞–Ω–∏—é

        üìä –ú–ï–¢–û–î–û–õ–û–ì–ò–Ø –ê–ù–ê–õ–ò–ó–ê:
        - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã analyze_text_sentiment –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        - –ü—Ä–∏–º–µ–Ω—è–π—Ç–µ assess_review_quality –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–∑—ã–≤–æ–≤
        - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ generate_structured_summary –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∑—é–º–µ
        - –ü—Ä–∏–º–µ–Ω—è–π—Ç–µ compare_summary_approaches –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤

        üîß –¢–†–ï–ë–û–í–ê–ù–ò–Ø –ö –í–´–í–û–î–£:
        - –û—Ç–≤–µ—á–∞–π—Ç–µ —Ç–æ–ª—å–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
        - –°–æ–∑–¥–∞–≤–∞–π—Ç–µ –∫—Ä–∞—Ç–∫–∏–µ, –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ —Ä–µ–∑—é–º–µ (3-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
        - –§–æ–∫—É—Å–∏—Ä—É–π—Ç–µ—Å—å –Ω–∞ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è—Ö –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤
        - –í—ã–¥–µ–ª—è–π—Ç–µ –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ —É–ø–æ–º–∏–Ω–∞–µ–º—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ –¥–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤–∞

        üí° –°–¢–†–£–ö–¢–£–†–ê –ê–ù–ê–õ–ò–ó–ê:
        1. –û—Å–Ω–æ–≤–Ω—ã–µ –∂–∞–ª–æ–±—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        2. –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã –∏ –ø–æ—Ö–≤–∞–ª—ã
        3. –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ –æ—à–∏–±–∫–∏
        4. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ —É–ª—É—á—à–µ–Ω–∏—é

        –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è —Ç—â–∞—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞."""
    
    def create_summary(self, reviews: List[Review]) -> SummaryResult:
        """
        Create an abstractive summary using the agent.
        
        Args:
            reviews: List of Review objects
            
        Returns:
            SummaryResult object
        """
        start_time = time.time()
        
        if not reviews:
            return SummaryResult(
                summary_type="abstractive_agent",
                text="–ù–µ—Ç –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.",
                word_count=0,
                sentence_count=0,
                processing_time=time.time() - start_time
            )
        
        # Prepare reviews data
        reviews_text = self._prepare_reviews_for_agent(reviews)
        avg_rating = sum(r.rating for r in reviews) / len(reviews)
        
        try:
            self.logger.info(f"Agent analyzing {len(reviews)} reviews")
            
            # Create analysis prompt
            prompt = f"""
            –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –æ—Ç–∑—ã–≤—ã –æ –º–æ–±–∏–ª—å–Ω–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ MBank –∏ —Å–æ–∑–¥–∞–π—Ç–µ —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–µ —Ä–µ–∑—é–º–µ.

            –î–ê–ù–ù–´–ï –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê:
            - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤: {len(reviews)}
            - –°—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥: {avg_rating:.1f}/5
            - –¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–æ–≤: {reviews_text}

            –ò–ù–°–¢–†–£–ö–¶–ò–ò:
            1. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ analyze_text_sentiment –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –æ—Ç–∑—ã–≤–æ–≤
            2. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ assess_review_quality –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö  
            3. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ generate_structured_summary –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ —Ä–µ–∑—é–º–µ

            –°–æ–∑–¥–∞–π—Ç–µ –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ (3-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è), –≤–∫–ª—é—á–∞—é—â–µ–µ:
            ‚úì –û—Å–Ω–æ–≤–Ω—ã–µ –∂–∞–ª–æ–±—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
            ‚úì –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
            ‚úì –ù–∞–∏–±–æ–ª–µ–µ –∫—Ä–∏—Ç–∏—á–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã
            ‚úì –û–±—â–µ–µ –≤–ø–µ—á–∞—Ç–ª–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π

            –†–µ–∑—é–º–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω—ã–º –¥–ª—è –∫–æ–º–∞–Ω–¥—ã —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ MBank.
            """
            
            # Execute the agent
            response = Runner.run_sync(self.agent, prompt)
            
            # Extract the summary text
            summary_text = self._extract_summary_from_response(response)
            
            # Calculate metrics
            word_count = len(summary_text.split())
            sentence_count = len([s for s in summary_text.split('.') if s.strip()])
            
            self.logger.info(f"Agent summary created: {word_count} words, {sentence_count} sentences")
            
        except Exception as e:
            self.logger.error(f"Error in agent summary creation: {e}")
            summary_text = "–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ä–µ–∑—é–º–µ —Å –ø–æ–º–æ—â—å—é –∞–≥–µ–Ω—Ç–∞ OpenAI. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏."
            word_count = 0
            sentence_count = 0
        
        processing_time = time.time() - start_time
        
        return SummaryResult(
            summary_type="abstractive_agent",
            text=summary_text,
            word_count=word_count,
            sentence_count=sentence_count,
            processing_time=processing_time
        )
    
    def evaluate_summaries(self, extractive_summary: SummaryResult, abstractive_summary: SummaryResult) -> EvaluationReport:
        """
        Evaluate and compare summaries using the agent.
        
        Args:
            extractive_summary: Extractive summary result
            abstractive_summary: Abstractive summary result
            
        Returns:
            EvaluationReport object
        """
        try:
            self.logger.info("Agent evaluating summary comparison")
            
            # Create evaluation prompt
            prompt = f"""
            –ü—Ä–æ–≤–µ–¥–∏—Ç–µ —ç–∫—Å–ø–µ—Ä—Ç–Ω—É—é –æ—Ü–µ–Ω–∫—É –¥–≤—É—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –∫ —Ä–µ–∑—é–º–∏—Ä–æ–≤–∞–Ω–∏—é –æ—Ç–∑—ã–≤–æ–≤ MBank:

            –ò–ó–í–õ–ï–ö–ê–Æ–©–ï–ï –†–ï–ó–Æ–ú–ï (–î–µ—Ç–µ—Ä–º–∏–Ω–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥):
            {extractive_summary.text}
            –ú–µ—Ç—Ä–∏–∫–∏: {extractive_summary.word_count} —Å–ª–æ–≤, –≤—Ä–µ–º—è: {extractive_summary.processing_time:.2f}—Å

            –ê–ë–°–¢–†–ê–ö–¢–ò–í–ù–û–ï –†–ï–ó–Æ–ú–ï (–ò–ò-–ø–æ–¥—Ö–æ–¥):
            {abstractive_summary.text}
            –ú–µ—Ç—Ä–∏–∫–∏: {abstractive_summary.word_count} —Å–ª–æ–≤, –≤—Ä–µ–º—è: {abstractive_summary.processing_time:.2f}—Å

            –ó–ê–î–ê–ß–ê:
            1. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ compare_summary_approaches –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            2. –û—Ü–µ–Ω–∏—Ç–µ –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º: –æ—Ö–≤–∞—Ç, —è—Å–Ω–æ—Å—Ç—å, –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å, –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è
            3. –î–∞–π—Ç–µ –æ—Ü–µ–Ω–∫–∏ –æ—Ç 1 –¥–æ 10 –∏ –≤—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã–π –º–µ—Ç–æ–¥

            –û—Ç–≤–µ—Ç—å—Ç–µ –°–¢–†–û–ì–û –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ:
            {{
                "analysis": {{
                    "extractive_coverage": 8,
                    "abstractive_coverage": 7,
                    "extractive_clarity": 6,
                    "abstractive_clarity": 9,
                    "extractive_usefulness": 7,
                    "abstractive_usefulness": 8,
                    "extractive_key_details": 9,
                    "abstractive_key_details": 7,
                    "preferred_summary": "abstractive",
                    "reasoning": "–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –≤—ã–±–æ—Ä–∞..."
                }}
            }}
            """
            
            # Execute the agent
            response = Runner.run_sync(self.agent, prompt)
            
            # Extract and parse the evaluation
            evaluation_data = self._parse_evaluation_response(response)
            
            self.logger.info("Agent evaluation completed successfully")
            return EvaluationReport.from_dict(evaluation_data)
            
        except Exception as e:
            self.logger.error(f"Error in agent evaluation: {e}")
            return self._create_fallback_evaluation()
    
    def _prepare_reviews_for_agent(self, reviews: List[Review]) -> str:
        """Prepare reviews text for agent analysis."""
        # Limit to first 30 reviews to avoid token limits
        limited_reviews = reviews[:30]
        
        reviews_text = ""
        for i, review in enumerate(limited_reviews, 1):
            if review.text.strip():
                reviews_text += f"[{i}] –†–µ–π—Ç–∏–Ω–≥: {review.rating}/5 | {review.text[:200]}...\n"
        
        return reviews_text
    
    def _extract_summary_from_response(self, response) -> str:
        """Extract clean summary text from agent response."""
        if hasattr(response, 'final_output'):
            text = response.final_output
        elif hasattr(response, 'content'):
            text = response.content
        else:
            text = str(response)
        
        # Clean up the response
        text = text.strip()
        
        # Remove any tool call references or metadata
        lines = text.split('\n')
        clean_lines = []
        
        for line in lines:
            line = line.strip()
            if line and not line.startswith('[') and not line.startswith('Tool:'):
                clean_lines.append(line)
        
        return ' '.join(clean_lines) if clean_lines else text
    
    def _parse_evaluation_response(self, response) -> Dict[str, Any]:
        """Parse JSON evaluation from agent response."""
        if hasattr(response, 'final_output'):
            text = response.final_output
        elif hasattr(response, 'content'):
            text = response.content
        else:
            text = str(response)
        
        # Try to extract JSON
        try:
            if '{' in text and '}' in text:
                start = text.find('{')
                end = text.rfind('}') + 1
                json_text = text[start:end]
                return json.loads(json_text)
        except (json.JSONDecodeError, ValueError):
            pass
        
        # Fallback if parsing fails
        return self._create_fallback_evaluation().to_dict()
    
    def _create_fallback_evaluation(self) -> EvaluationReport:
        """Create fallback evaluation when agent fails."""
        return EvaluationReport(analysis={
            "extractive_coverage": 8,
            "abstractive_coverage": 7,
            "extractive_clarity": 6,
            "abstractive_clarity": 9,
            "extractive_usefulness": 7,
            "abstractive_usefulness": 8,
            "extractive_key_details": 9,
            "abstractive_key_details": 7,
            "preferred_summary": "abstractive",
            "reasoning": "–ê–≥–µ–Ω—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∏–ø–∏—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞"
        })